#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# This script is designed to work with the training data from kaggle.com's
# 'dato-native' competition.  It has only been tested on machines with at least
# 64 GB RAM, and is unlikely to work with less than 32 GB RAM.  On an Intel
# i7-3930K, this script runs in about 32 minutes.
#
# The model generated by this script scores 0.98510 on the private leaderboard.
#
# Copyright Â© Morten Hustveit, 2015

import argparse
import csv
import os
import subprocess
import sys
import tempfile
import zipfile

parser = argparse.ArgumentParser(description='kaggle.com dato-native example script')
parser.add_argument('--classifier-path', metavar='classifier_path', type=str, nargs='?',
                    help='path of text-classifier',
                    default='/usr/local/bin/text-classifier')
parser.add_argument('--work-dir', metavar='work_dir', type=str, nargs='?',
                    help='directory in which to put temporary files',
                    default='.')
args = parser.parse_args()

reader = csv.reader(open('data/train_v2.csv', 'r'))
header = reader.next()

classes = {}

for row in reader:
  classes[row[0]] = int(row[1])

reader = csv.reader(open('data/sampleSubmission_v2.csv', 'r'))
header = reader.next()

test_files = set()

for row in reader:
  test_files.add(row[0])

data_path = os.path.join(args.work_dir, 'training-data')
model_path = os.path.join(args.work_dir, 'model')

process = subprocess.Popen([args.classifier_path, 'batch-learn', data_path], stdin=subprocess.PIPE)

for archive_path in ['data/0.zip', 'data/1.zip', 'data/2.zip', 'data/3.zip', 'data/4.zip']:
  sys.stderr.write('%s\n' % (archive_path))
  with open(archive_path, 'r') as archive_file:
    archive = zipfile.ZipFile(archive_file, 'r')

    for name in archive.namelist():
      basename = os.path.basename(name)
      if basename not in classes:
        continue

      data = archive.read(name)
      if 0 == len(data):
        continue

      class_id = classes[basename]

      process.stdin.write('%d %d\n' % (class_id, len(data)))
      process.stdin.write(data)

process.stdin.close()
process.wait()

# The `--C` and `--weight-ratio` parameters are selected automatically if not
# provided on the command line.  This process can take many hours, but the
# parameters can be reused even with minor model changes later.

process = subprocess.Popen([
  args.classifier_path,
  '--weight=bns',
  '--C=0.2257',
  '--weight-ratio=0.5816',
  '--epsilon=1.0',
  '--threshold=2',
  'analyze',
  data_path,
  model_path
  ])
process.communicate()

results = tempfile.TemporaryFile()
process = subprocess.Popen([args.classifier_path, 'batch-classify', model_path], stdin=subprocess.PIPE, stdout=results)

result_array = []

for archive_path in ['data/5.zip']:
  with open(archive_path, 'r') as archive_file:
    archive = zipfile.ZipFile(archive_file, 'r')

    for name in archive.namelist():
      basename = os.path.basename(name)

      if basename not in test_files:
        continue

      data = archive.read(name)

      if len(data) == 0:
        result_array.append((basename, 0.0))
        continue

      process.stdin.write('%s %d\n' % (basename, len(data)))
      process.stdin.write(data)

process.stdin.close()
process.wait()

results.seek(0)

min_score = 0
max_score = 0

for line in results:
  row = line.split('\t')
  score = float(row[1])
  min_score = min(score, min_score)
  max_score = max(score, max_score)
  result_array.append((row[0], score))

with open('submission.csv', 'w') as out:
  out.write('file,sponsored\n')
  for result in result_array:
    out.write('%s,%.9g\n' % (result[0], (result[1] - min_score) / (max_score - min_score)))
